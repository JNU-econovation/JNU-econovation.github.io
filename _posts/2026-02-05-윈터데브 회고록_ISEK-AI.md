---
layout: post
title: "[2026 WINTER DEV] 나만의 가상 캐릭터와 실시간으로 소통하는 버츄얼 AI 음성채팅 서비스 'ISEK-AI', ISEK-AI팀"
subtitle: "[2026 WINTER DEV] 나만의 가상 캐릭터와 실시간으로 소통하는 버츄얼 AI 음성채팅 서비스 'ISEK-AI', ISEK-AI팀"
author: taeeun
categories: "SUMMER/WINTER_DEV"
banner:
  image: "https://velog.velcdn.com/images/reuscap/post/7363f85f-6677-4eb9-b380-9dba25128a5f/image.png"
tags: "dev"
sidebar: []
---

## [2026 WINTER DEV] 나만의 가상 캐릭터와 실시간으로 소통하는 버츄얼 AI 음성채팅 서비스 'ISEK-AI', ISEK-AI팀"

### 프로젝트 소개

<img src="https://velog.velcdn.com/images/reuscap/post/7363f85f-6677-4eb9-b380-9dba25128a5f/image.png" alt="ISEK-AI 대표 사진" />
<img src="https://velog.velcdn.com/images/reuscap/post/fc3cfe7c-988e-4bae-95da-9a60c1269547/image.png" alt="ISEK-AI 중간 사진" />

ISEK-AI는 '이세계(ISEKAI)'와 '이색적인 AI'의 중의적인 뜻에서 따온 이름으로, 자신만의 가상 캐릭터를 현실로 구현하고 소통할 수 있는 AI 컴페니언 서비스입니다. 단순한 대화형 챗봇을 넘어 사용자가 직접 Live2D 외형과 목소리를 생성하여 캐릭터에 생명력을 불어넣고, 실시간 교감은 물론 생성한 캐릭터를 다른 사용자들과 공유하며 함께 즐길 수 있는 사용자 참여형 캐릭터 생태계 구축을 주요 목표로 합니다.

<br/>

### 팀원 소개

<img src="https://velog.velcdn.com/images/reuscap/post/12634f86-0c97-441c-942b-42a814120fd1/image.png" alt="ISEK-AI 팀원 사진"/>

ISEK-AI팀은 AI 영현님, AI 준현님, BE 수랑님, FE 준서님, FE 태은님으로 구성되어 있습니다!

<br/>

### 인터뷰

**Q. 프로젝트를 소개해주세요!**

(영현) ISEK-AI는 기존의 정적인 텍스트 및 단순 이미지 중심 AI 인터랙션의 한계를 극복하고, 사용자에게 실재감 있는 몰입 경험을 제공하기 위해 개발된 동적 비주얼 기반 AI 캐릭터 창작 플랫폼입니다.

생동감 없는 대화 경험과 복잡한 기술적 장벽으로 인한 캐릭터 제작의 어려움을 해결하고자 기획되었으며, 고도화된 모션 생성 기술과 직관적인 저작 도구를 통해 누구나 쉽게 움직이는 나만의 AI 페르소나를 구현할 수 있도록 지원합니다. 사용자는 시각적으로 살아 움직이는 이세계 캐릭터와 깊이 있게 교감하고, 창작물을 커뮤니티에 공유하며 상호작용의 범위를 확장할 수 있습니다.

<br/>

**Q. 프로젝트 하면서 어떤 문제를 겪었나요?**

(영현) ISEK-AI의 핵심인 '살아 움직이는 AI 캐릭터'를 구현하기 위해 Live2D Cubism SDK를 도입하는 과정에서 기술적 진입장벽을 겪었습니다.

기존에 사용했던 대중적인 라이브러리들은 풍부한 한글/영문 레퍼런스와 친절한 공식 문서를 제공했지만, Live2D SDK는 상대적으로 사용자 층이 좁고 공식 문서의 상당 부분이 일본어로 되어 있어 초기 로직 파악에 큰 어려움이 있었습니다. 특히 캐릭터를 화면에 렌더링하고, 특정 트리거에 맞춰 움직임을 제어하는 메서드와 클래스 구조를 이해하는 데 있어 기존의 학습 방식으로는 한계가 있었습니다.

이 문제를 해결하기 위해 Gemini CLI와 GitHub Copilot 같은 최신 AI 도구를 적극적으로 활용했습니다. 파편화된 일본어 문서를 AI로 분석하여 핵심 워크플로우를 추출했고, 생소한 메서드의 역할과 클래스 간의 관계를 Copilot과의 인터랙션을 통해 하나씩 검증해 나갔습니다. 이러한 과정을 반복하며 '어떤 클래스가 모델을 로드하고, 어떤 메서드가 실시간 움직임을 담당하는지'에 대한 기술적 구조를 체계적으로 파악할 수 있었고, 결과적으로 의도했던 생동감 넘치는 캐릭터 인터랙션을 성공적으로 구현해냈습니다.

이 경험을 통해 낯선 기술 스택이나 불친절한 개발 환경을 마주하더라도, 최신 AI 도구를 전략적으로 활용해 문제를 정의하고 빠르게 학습해내는 '기술 적응력'의 중요성을 깨달았습니다. 단순히 익숙한 도구에 안주하지 않고, 필요하다면 어떤 수단을 써서라도 최선의 결과를 만들어내는 개발자로서의 태도를 배울 수 있었습니다.

<br/>

**Q. 프로젝트를 하기 전 후 달라진 점이 있다면?**

(영현) 이번 프로젝트를 통해 철저한 사전 조사의 중요성을 깊이 깨달았습니다.

처음에는 빠른 구현에 집중하느라 TTS 모델을 깊은 고민 없이 선택했습니다. 하지만 개발 중반에 더 우수한 성능의 모델들을 발견하게 되었고, 초기에 조금 더 신중하게 비교 분석했다면 서비스의 퀄리티를 더 높일 수 있었을 것이라는 아쉬움을 느꼈습니다.

Live2D 구현 과정에서도 혼자 고군분투하며 많은 시간을 보냈으나, 나중에야 참고할 수 있는 훌륭한 오픈소스 프로젝트들이 많다는 것을 알게 되었습니다. 만약 미리 기술 생태계를 조사했다면 시행착오를 줄이고 더 정교한 기능을 만드는 데 집중할 수 있었을 것입니다.

이러한 경험을 통해 무작정 개발에 착수하기보다, 최적의 기술 스택을 찾는 사전 조사가 전체적인 개발 효율과 완성도를 결정한다는 점을 배웠습니다. 현재는 어떤 기능을 도입하든 충분한 레퍼런스 조사를 선행하는 습관을 갖게 되었습니다.

<br/>

**Q. 프로젝트를 시작하는 팀에게 전해줄 꿀팁을 말해주세요!**

(승희) 완벽한 기획보다 빠른 실행과 피드백 수용이 핵심이라는 말을 전하고 싶습니다. 모든 기능을 다 넣으려 하기보다, 사용자가 진짜 원하는 기능 하나를 먼저 잘 구현하고 테스트해보는 것이 더 중요했습니다. 그리고 꼭! 피드백을 받을 수 있는 구조를 미리 만들어두세요. 동아리원들의 조언은 물론, 간단한 설문이나 인터뷰 방식이라도 준비해두면 사용자 반응을 놓치지 않고 바로 반영할 수 있습니다.

<br/>

**Q. 프로젝트 개발 중 어려움을 겪은 경험이 있나요? 어떻게 해결했으며, 그 과정에서 얻은 교훈은 무엇인가요?** 

(영현) 제가 생각하는 가장 중요한 건 초반에 그냥 질릴 정도로 대화를 많이 나누는 것입니다. "이 정도면 다들 알겠지" 하고 대충 넘어가는 게 나중에 정말 큰 독이 되기 때문입니다.

개발이 어느 정도 진행된 시점에 "어? 나는 이런 의도로 말한 게 아니었는데" 같은 소리가 나오는 순간, 기획을 뒤엎고 코드를 갈아엎는 지옥이 시작됩니다. 사실 그 고통에 비하면 초반에 모여서 입 아프게 떠드는 건 정말 아무것도 아닙니다. 지금 좀 귀찮더라도 사소한 것까지 꼼꼼하게 합을 맞춰두는 것이, 결국 마감 직전에 피눈물 흘리며 밤샘하는 걸 막아주는 가장 확실한 방법이라고 생각합니다.


<br/>

**Q. 프로젝트의 기술적인 도전과제나 혁신적인 부분은 무엇이었나요?** 

(수랑) 본 프로젝트는 사용자가 "실시간 음성 통화에 가까운 상호작용"을 수행하도록 하는 기술적 도전과제가 있었습니다.

실시간 음성 통화와 유사한 사용자 경험을 제공하기 위해, "사용자가 지연 시간을 인지하지 않도록 하는 것이 기술적 요구사항"이었으며, 이를 해결하기 위해 여러 가지 접근 방식을 검토 및 구현하였습니다.

1. STT–LLM–TTS 파이프라인 방식

첫 번째로, 사용자 음성을 STT(Speech-to-Text)를 통해 텍스트로 변환한 후 Gemini REST API를 이용해 응답을 생성하고, 이를 다시 TTS(Text-to-Speech)로 음성 출력하는 구조입니다.

해당 방식은 구현 난이도가 낮고 구조가 단순하다는 장점이 있으나, STT 및 TTS 과정에서 발생하는 병목으로 인해 지연 시간이 발생하고, 음성을 텍스트로 변환하는 과정에서 비언어적 컨텍스트가 상당 부분 유실되는 문제가 확인되었습니다.

이로 인해 실제 통화와 유사한 사용자 경험을 제공하기에는 한계가 있었습니다.

2. 단일 Native Audio LLM 사용 방식

다음으로, 사용자 음성을 직접 입력으로 받아 처리하는 `Gemini Flash 2.5 Native Audio` 모델을 단일 LLM으로 사용하는 방식으로 구현하였습니다.

이 방식은 음성을 직접 처리함으로써 지연 시간을 단축하고, 음성에 포함된 정보를 보다 풍부하게 활용할 수 있다는 장점이 있었습니다.

그러나 단일 모델이 다음과 같은 역할을 모두 수행하게 되었습니다.

- 사용자 발화에 대해 응답 여부 판단
- 대화의 단기, 중기기억
- RAG 기반 외부 정보 검색
- 감정 및 상황 해석
- 페르소나
- 실제 응답 생성

이로 인해 모델의 책임이 과도하게 집중되었으며, 엉뚱한 응답을 하거나, 과도한 처리로 인해 Gemini 서버 내에서 WebSocket 오류가 발생하는 등 시스템 전반의 안정성이 저하되는 문제가 발생하였습니다.

3. LLM Cascade 아키텍처 적용

따라서 저희는 위 문제를 해결하기 위해, 불과 dev 5일 전에 모델의 역할과 책임을 단계별로 분리하는 LLM Cascade 아키텍처로 리팩토링 하였습니다.
구조는 다음과 같습니다.

**1단계 LLM (Routing Model)**

사용자 음성을 입력으로 받아, 해당 상황에서 AI가 **응답을 수행해야 하는지 여부를 판단**한다.

**2단계 LLM (Response Generation Model)**

1단계에서 응답이 필요하다고 판단된 경우에만 호출되며,
대화 맥락, 외부 정보 검색(RAG), 표현 방식 등을 종합하여
실제로 "대답을 어떻게 할지"를 결정한다.

본 구조는 사전에 특정 아키텍처를 목표로 설계된 것이 아니라, 실시간 음성 기반 생성형 AI라는 문제를 반복적으로 실험하고 개선하는 과정에서 자연스럽게 도출된 결과라는 점에서 의미가 있다고 생각합니다.

<br/>

**Q. 개발자로서의 역량 향상을 위해 어떤 노력을 기울였으며, 이 프로젝트를 통해 어떤 기술적 성장을 이루었나요?** 

(태은) 저는 이번 학기에 처음으로 프로젝트에서 웹개발을 맡게 되었습니다. 이전에 Vanilla.js를 사용해본 적이 있어서, 초기에 프로젝트가 Vanilla.js로 진행될 때는 제가 알고 있던 문법들을 활용해 CSS 문법으로 디자인을 수정하고 JS 기능을 구현하며 기초를 다졌습니다. 하지만 프로젝트가 진행됨에 따라 WebSocket 및 Live2D 렌더링 같은 실시간 상호작용 기능을 더욱 안정적으로 유지하고, 컴포넌트화를 통해 코드의 유지보수성을 높이기 위해 React로의 마이그레이션을 결정했습니다. React 프레임워크는 처음이라 초기에는 낯선 환경이었지만, 컴포넌트 중심의 설계 방식을 익히며 점차 적응해 나갔습니다. 특히 이번 프로젝트에서 가장 큰 도전이었던 WebSocket 적용은 React의 Hook을 활용하여 컴포넌트의 생명주기에 맞춰 연결과 해제를 체계적으로 관리하는 법을 배우는 계기가 되었습니다. 실시간으로 들어오는 데이터를 useState를 통해 상태로 관리하고, 이를 UI에 즉각적으로 반영하는 구조를 설계하며 비동기 통신에 대한 기술적 이해도를 높일 수 있었습니다. 또한, 이전 프로젝트에서는 앱 개발을 맡아 혼자서 모든 과정을 진행했기에 Git을 통한 협업 경험이 상대적으로 부족했었습니다. 하지만 이번 프로젝트에서는 3명의 팀원들과 함께 브랜치 전략을 수립하고 코드 리뷰와 충돌 해결 과정을 거치며 협업 도구로서의 Git을 훨씬 자세히 이해하게 되었습니다. 이번 프로젝트를 통해 React와 git에 대해 더 자세히 공부할 수 있는 좋은 경험이 되었습니다. 

<br/>

**Q. 팀 내 협업에서 문제는 없었는지 궁금합니다!**

(준서) 팀 내 협업 과정에서 가장 큰 문제는 프로젝트의 출발점이 명확하지 않았다는 점이었습니다. 문제 상황을 먼저 정의하고 그에 따른 해결 방향으로 기획을 진행하기보다는, 팀원들이 구현해보고자 했던 개발 요소를 중심으로 기획이 이루어졌습니다. 이로 인해 프로젝트 전반을 잡아주는 큰 틀과 공통된 방향성이 부족한 상태로 개발이 시작되었습니다.
<br/><br/>
이러한 구조 속에서 핵심 목표였던 ‘Live2D 모델과 음성을 활용한 대화 기능’은 비교적 빠르게 구현되었으나, 이후 콘텐츠 확장 단계에서 방향성에 대한 논의가 길어졌습니다. 팀원마다 프로젝트를 통해 추구하는 목표가 서로 달랐고, 다양한 의견을 최대한 반영하려는 과정에서 기획이 쉽게 좁혀지지 않았습니다. 또한 개발 기간이 충분히 남아 있지 않은 상황에서 여러 기획을 동시에 구현하기에는 시간적인 한계가 존재했습니다.
<br/><br/>
그 결과 논의 과정에서 제안되었던 다양한 콘텐츠를 실제 결과물로 모두 구현하지는 못했고, 핵심 기능 중심의 프로젝트로 마무리되었습니다.

<br/>

**Q. 문제에 대한 개선 방향은 어떤 것이 있을까요?** 

(준서) 이에 따라 향후 유사한 프로젝트를 진행할 경우, 초기 기획 단계에서 프로젝트의 목적과 범위를 명확히 설정하고 팀 내 공통된 목표를 합의하는 과정이 필요하다는 점이 드러났습니다. 또한 핵심 기능 구현 이후를 고려한 단계별 기획과 우선순위 설정을 통해 제한된 개발 기간 내에서도 보다 다양한 콘텐츠를 안정적으로 구현할 수 있을 것으로 판단되었습니다. 추후 기회가 주어진다면, 이번 프로젝트에서 논의에 그쳤던 다양한 콘텐츠를 추가하여 프로젝트를 확장해보고자 합니다.

<br/>
<br/>

지금까지 자신만의 가상 캐릭터를 현실로 구현하고 소통할 수 있는 AI 서비스 "ISEK-AI"를 기획한 "ISEK-AI"팀이었습니다. 앞으로도 재미있고 획기적인 기획의 서비스를 많이 만들어 나가길 기대하겠습니다!
